# -*- coding: utf-8 -*-
"""webscrapingproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MOGRYaVFtlXvj0apPxzxmz-h8uX5dBKN
"""

import requests
from bs4 import BeautifulSoup
import csv
import time
from urllib.parse import urljoin
from google.colab import files

# Base URL
BASE_URL = "https://books.toscrape.com/catalogue/"

RATING_MAP = {
    "One": "1",
    "Two": "2",
    "Three": "3",
    "Four": "4",
    "Five": "5"
}

def scrape_books(file_name):

    all_books = []

    print("Starting Book Scraper...\n")

    for page_number in range(1, 51):

        url = BASE_URL + f"page-{page_number}.html"
        print(f"Scraping Page {page_number}: {url}")

        response = requests.get(url)

        if response.status_code != 200:
            print("Page not found. Stopping.\n")
            break

        soup = BeautifulSoup(response.text, "lxml")
        products = soup.find_all("article", class_="product_pod")

        if not products:
            print("No books on this page. Ending.")
            break

        for product in products:

            title = product.h3.find("a")["title"]
            price = product.find("p", class_="price_color").text

            rating_class = product.find("p")["class"][1]
            rating = RATING_MAP.get(rating_class, "NA")

            stock_text = product.find("p", class_="instock availability").text.strip()

            partial_link = product.h3.find("a")["href"]
            full_link = urljoin(BASE_URL, partial_link)

            details_res = requests.get(full_link)
            details_soup = BeautifulSoup(details_res.text, "lxml")

            desc_div = details_soup.find("div", id="product_description")
            description = (
                desc_div.find_next("p").text.strip()
                if desc_div else "No description available"
            )

            category = details_soup.find("ul", class_="breadcrumb").find_all("li")[2].text.strip()
            product_code = details_soup.find("th", string="UPC").find_next("td").text.strip()

            all_books.append([
                title, price, rating, stock_text, category, description, product_code, full_link
            ])

            time.sleep(0.5)

    file = f"{file_name}.csv"
    with open(file, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["Title", "Price", "Rating", "Availability", "Category", "Description", "Product Code", "Link"])
        writer.writerows(all_books)

    print("\nFile saved successfully!")
    files.download(file)

scrape_books("books_data")